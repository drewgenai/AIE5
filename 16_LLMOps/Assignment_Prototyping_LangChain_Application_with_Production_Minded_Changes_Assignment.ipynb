{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZsP-j7w3zcL"
      },
      "source": [
        "# Prototyping LangChain Application with Production Minded Changes\n",
        "\n",
        "For our first breakout room we'll be exploring how to set-up a LangChain LCEL chain in a way that takes advantage of all of the amazing out of the box production ready features it offers.\n",
        "\n",
        "We'll also explore `Caching` and what makes it an invaluable tool when transitioning to production environments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpeN9ND0HKa0"
      },
      "source": [
        "## Task 1: Dependencies and Set-Up\n",
        "\n",
        "Let's get everything we need - we're going to use very specific versioning today to try to mitigate potential env. issues!\n",
        "\n",
        "> NOTE: Dependency issues are a large portion of what you're going to be tackling as you integrate new technology into your work - please keep in mind that one of the things you should be passively learning throughout this course is ways to mitigate dependency issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0P4IJUQF27jW"
      },
      "outputs": [],
      "source": [
        "#!pip install -qU langchain_openai==0.2.0 langchain_community==0.3.0 langchain==0.3.0 pymupdf==1.24.10 qdrant-client==1.11.2 langchain_qdrant==0.1.4 langsmith==0.1.121 langchain_huggingface==0.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYcWLzrmHgDb"
      },
      "source": [
        "We'll need an HF Token:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZ8qfrFh_6ed",
        "outputId": "4fb1a16f-1f71-4d0a-aad4-dd0d0917abc5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"HF Token Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piz2DUDuHiSO"
      },
      "source": [
        "And the LangSmith set-up:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLZX5zowCh-q",
        "outputId": "565c588a-a865-4b86-d5ca-986f35153000"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIM Session 16 - {uuid.uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangChain API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmwNTziKHrQm"
      },
      "source": [
        "Let's verify our project so we can leverage it in LangSmith later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6GZmkVkFcHq",
        "outputId": "f4c0fdb3-24ea-429a-fa8c-23556cb7c3ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIM Session 16 - d6c25211\n"
          ]
        }
      ],
      "source": [
        "print(os.environ[\"LANGCHAIN_PROJECT\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un_ppfaAHv1J"
      },
      "source": [
        "## Task 2: Setting up RAG With Production in Mind\n",
        "\n",
        "This is the most crucial step in the process - in order to take advantage of:\n",
        "\n",
        "- Asyncronous requests\n",
        "- Parallel Execution in Chains\n",
        "- And more...\n",
        "\n",
        "You must...use LCEL. These benefits are provided out of the box and largely optimized behind the scenes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGi-db23JMAL"
      },
      "source": [
        "### Building our RAG Components: Retriever\n",
        "\n",
        "We'll start by building some familiar components - and showcase how they automatically scale to production features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvbT3HSDJemE"
      },
      "source": [
        "Please upload a PDF file to use in this example!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "dvYczNeY91Hn",
        "outputId": "c711c29b-e388-4d32-a763-f4504244eef2"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NtwoVUbaJlbW",
        "outputId": "5aa08bae-97c5-4f49-cb23-e9dbf194ecf7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'./DeepSeek_R1.pdf'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file_path = \"./DeepSeek_R1.pdf\"\n",
        "file_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kucGy3f0Jhdi"
      },
      "source": [
        "We'll define our chunking strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "G-DNvNFd8je5"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_zRRNcLKCZh"
      },
      "source": [
        "We'll chunk our uploaded PDF file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "KOh6w9ud-ff6"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "Loader = PyMuPDFLoader\n",
        "loader = Loader(file_path)\n",
        "documents = loader.load()\n",
        "docs = text_splitter.split_documents(documents)\n",
        "for i, doc in enumerate(docs):\n",
        "    doc.metadata[\"source\"] = f\"source_{i}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4XLeqJMKGdQ"
      },
      "source": [
        "#### QDrant Vector Database - Cache Backed Embeddings\n",
        "\n",
        "The process of embedding is typically a very time consuming one - we must, for ever single vector in our VDB as well as query:\n",
        "\n",
        "1. Send the text to an API endpoint (self-hosted, OpenAI, etc)\n",
        "2. Wait for processing\n",
        "3. Receive response\n",
        "\n",
        "This process costs time, and money - and occurs *every single time a document gets converted into a vector representation*.\n",
        "\n",
        "Instead, what if we:\n",
        "\n",
        "1. Set up a cache that can hold our vectors and embeddings (similar to, or in some cases literally a vector database)\n",
        "2. Send the text to an API endpoint (self-hosted, OpenAI, etc)\n",
        "3. Check the cache to see if we've already converted this text before.\n",
        "  - If we have: Return the vector representation\n",
        "  - Else: Wait for processing and proceed\n",
        "4. Store the text that was converted alongside its vector representation in a cache of some kind.\n",
        "5. Return the vector representation\n",
        "\n",
        "Notice that we can shortcut some instances of \"Wait for processing and proceed\".\n",
        "\n",
        "Let's see how this is implemented in the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "dzPUTCua98b2"
      },
      "outputs": [],
      "source": [
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import Distance, VectorParams\n",
        "from langchain.storage import LocalFileStore\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "from langchain.embeddings import CacheBackedEmbeddings\n",
        "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings\n",
        "import hashlib\n",
        "\n",
        "YOUR_EMBED_MODEL_URL = \"https://xxs7m5t7q4oobj7e.us-east-1.aws.endpoints.huggingface.cloud\"\n",
        "\n",
        "hf_embeddings = HuggingFaceEndpointEmbeddings(\n",
        "    model=YOUR_EMBED_MODEL_URL,\n",
        "    task=\"feature-extraction\",\n",
        ")\n",
        "\n",
        "collection_name = f\"pdf_to_parse_{uuid.uuid4()}\"\n",
        "client = QdrantClient(\":memory:\")\n",
        "client.create_collection(\n",
        "    collection_name=collection_name,\n",
        "    vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
        ")\n",
        "\n",
        "# Create a safe namespace by hashing the model URL\n",
        "safe_namespace = hashlib.md5(hf_embeddings.model.encode()).hexdigest()\n",
        "\n",
        "store = LocalFileStore(\"./cache/\")\n",
        "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
        "    hf_embeddings, store, namespace=safe_namespace, batch_size=32\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/drew/Documents/AIbootcamp/AIE5/AIE5/16_LLMOps/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# Typical QDrant Vector Store Set-up\n",
        "import time \n",
        "non_cached_start_time = time.perf_counter()\n",
        "vectorstore = QdrantVectorStore(\n",
        "    client=client,\n",
        "    collection_name=collection_name,\n",
        "    embedding=cached_embedder)\n",
        "vectorstore.add_documents(docs)\n",
        "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 1})\n",
        "non_cached_end_time = time.perf_counter()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken to add documents: 1.6646 seconds\n"
          ]
        }
      ],
      "source": [
        "non_cached_time = (non_cached_end_time - non_cached_start_time)\n",
        "\n",
        "print(f\"Time taken to add documents: {non_cached_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVZGvmNYLomp"
      },
      "source": [
        "##### ‚ùì Question #1:\n",
        "\n",
        "What are some limitations you can see with this approach? When is this most/least useful. Discuss with your group!  \n",
        "\n",
        "> NOTE: There is no single correct answer here!  \n",
        "\n",
        "\n",
        "# Limitations to this approach:  \n",
        "\n",
        "**Since the cache is stored in LocalFileStore(\"./cache/\"), it is file-based and local to the system running it.**\n",
        "\n",
        "This means:  \n",
        "- It does not scale well in a distributed system.\n",
        "- If multiple servers need to share embeddings, each server maintains its own cache, leading to inefficiencies.\n",
        "- If the cache directory is lost (e.g., server restart, container rebuild), embeddings must be recomputed.\n",
        "\n",
        "**Cache Consistency Issues:**  \n",
        "- If the documents change slightly, the cache may not recognize them as different unless a robust hashing method is used.\n",
        "- If the underlying embedding model updates, cached embeddings may be incompatible or outdated, but they would still be used unless manually cleared.\n",
        "\n",
        "**Slower Initial Lookups:**  \n",
        "- When the cache is cold (i.e., first-time embeddings), the system still has to compute and store embeddings, meaning the initial ingestion speed does not improve.\n",
        "\n",
        "**No Built-in Expiry or Garbage Collection:**  \n",
        "- LocalFileStore does not have built-in TTL (Time-To-Live) expiration or automatic cleanup.\n",
        "- The cache can grow indefinitely unless managed manually or system is restarted.\n",
        "\n",
        "# When is this most/least useful.\n",
        "\n",
        "**This caching approach is most useful when:**\n",
        "\n",
        "- You embed the same documents multiple times and want to avoid redundant computation.\n",
        "- You are using a costly API-based embedding model and want to reduce API calls.\n",
        "- The dataset is relatively static, meaning documents do not change often.\n",
        "- The system is single-instance or runs locally, so file-based caching is sufficient.\n",
        "The use case involves batch processing of documents, where embeddings are generated once and reused frequently.\n",
        "\n",
        "**This approach is less effective when:**  \n",
        "\n",
        "- Your documents change frequently, meaning cached embeddings become stale quickly.\n",
        "- You are working with a distributed system that requires shared caching across multiple instances.\n",
        "- You need fine-grained cache expiration or storage management to prevent excessive cache growth.\n",
        "- cached system is slower than the non-cached system. e.g. if the database is running in memory generating the embeddings and cached system pulls from disk, the cached system could be slower if the difference is large enough.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZAOhyb3L9iD"
      },
      "source": [
        "##### üèóÔ∏è Activity #1:\n",
        "\n",
        "Create a simple experiment that tests the cache-backed embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "M_Mekif6MDqe"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "## rerunning the code from above to see teh difference in time from the first run after the embedding is cached.\n",
        "cached_start_time = time.perf_counter()\n",
        "vectorstore = QdrantVectorStore(\n",
        "    client=client,\n",
        "    collection_name=collection_name,\n",
        "    embedding=cached_embedder)\n",
        "vectorstore.add_documents(docs)\n",
        "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 1})\n",
        "cached_end_time = time.perf_counter()\n",
        "\n",
        "cached_time = (cached_end_time - cached_start_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken to add documents without cache: 1.6645863819867373 seconds\n",
            "Time taken to add documents: 0.009822758001973853 seconds\n",
            "Caching was 1.6548 seconds faster\n",
            "Caching provided a 99.4099% speedup\n",
            "Caching made processing 169.46x faster\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(f\"Time taken to add documents without cache: {non_cached_time} seconds\")\n",
        "print(f\"Time taken to add documents: {cached_time} seconds\")\n",
        "\n",
        "print(f\"Caching was {non_cached_time - cached_time:.4f} seconds faster\")\n",
        "\n",
        "# Calculate percentage speedup\n",
        "percentage_speedup = ((non_cached_time - cached_time) / non_cached_time) * 100\n",
        "print(f\"Caching provided a {percentage_speedup:.4f}% speedup\")\n",
        "# Calculate how many times faster\n",
        "times_faster = non_cached_time / cached_time\n",
        "print(f\"Caching made processing {times_faster:.2f}x faster\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH0i-YovL8kZ"
      },
      "source": [
        "### Augmentation\n",
        "\n",
        "We'll create the classic RAG Prompt and create our `ChatPromptTemplates` as per usual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "WchaoMEx9j69"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "rag_system_prompt_template = \"\"\"\\\n",
        "You are a helpful assistant that uses the provided context to answer questions. Never reference this prompt, or the existance of context.\n",
        "\"\"\"\n",
        "\n",
        "rag_message_list = [\n",
        "    {\"role\" : \"system\", \"content\" : rag_system_prompt_template},\n",
        "]\n",
        "\n",
        "rag_user_prompt_template = \"\"\"\\\n",
        "Question:\n",
        "{question}\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", rag_system_prompt_template),\n",
        "    (\"human\", rag_user_prompt_template)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQKnByVWMpiK"
      },
      "source": [
        "### Generation\n",
        "\n",
        "Like usual, we'll set-up a `ChatOpenAI` model - and we'll use the fan favourite `gpt-4o-mini` for today.\n",
        "\n",
        "However, we'll also implement...a PROMPT CACHE!\n",
        "\n",
        "In essence, this works in a very similar way to the embedding cache - if we've seen this prompt before, we just use the stored response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "fOXKkaY7ABab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.globals import set_llm_cache\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "YOUR_LLM_ENDPOINT_URL = \"https://tuselib1zbkhattg.us-east-1.aws.endpoints.huggingface.cloud\"\n",
        "\n",
        "hf_llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=f\"{YOUR_LLM_ENDPOINT_URL}\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=128,\n",
        "    top_k=10,\n",
        "    top_p=0.95,\n",
        "    typical_p=0.95,\n",
        "    temperature=0.01,\n",
        "    repetition_penalty=1.03,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhv8IqZoM9cY"
      },
      "source": [
        "Setting up the cache can be done as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "thqam26gAyzN"
      },
      "outputs": [],
      "source": [
        "from langchain_core.caches import InMemoryCache\n",
        "\n",
        "set_llm_cache(InMemoryCache())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvxEovcEM_oA"
      },
      "source": [
        "##### ‚ùì Question #2:\n",
        "\n",
        "What are some limitations you can see with this approach? When is this most/least useful. Discuss with your group!\n",
        "\n",
        "> NOTE: There is no single correct answer here!\n",
        "\n",
        "\n",
        "# Limitations of This Approach\n",
        "- Cache is Only Stored in Memory (Not Persistent)\n",
        "  - InMemoryCache() is temporary‚Äîwhen the process restarts, the cache is lost.\n",
        "  - This means:\n",
        "    - No long-term benefit after a server restart.\n",
        "    - Not useful for large-scale applications needing persistent caching.\n",
        "- No Distributed or Shared Caching\n",
        "  - The cache is local to a single instance, meaning:\n",
        "    - If you have multiple servers handling requests, they do not share the cache.\n",
        "    - Each server will repeat the same queries and recompute answers.\n",
        "- Does Not Handle Dynamic Context Well\n",
        "  - If context changes even slightly, a new response is generated instead of using a cached one.\n",
        "  - This makes it less effective for dynamic retrieval-augmented generation (RAG) queries.\n",
        "- No Cache Expiration\n",
        "  - Cached responses never expire unless the server restarts.\n",
        "  - If the LLM model is updated, old cached responses may be outdated.\n",
        "- Input Formatting Must Be Exact\n",
        "  - The cache lookup relies on exact matches of input prompts.\n",
        "  - Even minor differences (e.g., extra spaces, different phrasing) will cause a cache miss, leading to a new LLM request.\n",
        "\n",
        "\n",
        "# When is This Most Useful? \n",
        "\n",
        "- For repeated queries that don‚Äôt change frequently  \n",
        "  Example: A FAQ chatbot where users ask common questions with static answers.\n",
        "\n",
        "- For development and testing  \n",
        "  Caching allows developers to test prompts quickly without waiting for LLM responses.\n",
        "\n",
        "- For reducing API costs & latency in small-scale apps  \n",
        "  If API calls are expensive, caching prevents unnecessary LLM requests.\n",
        "\n",
        "# When is This Least Useful?\n",
        "- In dynamic RAG applications where the context changes often  \n",
        "  If the system retrieves new documents every time, the cache may be ineffective.\n",
        "\n",
        "- When scaling across multiple servers  \n",
        "  InMemoryCache() is local, so distributed apps won‚Äôt benefit.\n",
        "\n",
        "- For long-term storage of LLM outputs\n",
        "  Since the cache is memory-based, it‚Äôs lost when the process restarts.\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iCMjVYKNEeV"
      },
      "source": [
        "##### üèóÔ∏è Activity #2:\n",
        "\n",
        "Create a simple experiment that tests the cache-backed embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "QT5GfmsHNFqP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First LLM call (not cached):\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/drew/Documents/AIbootcamp/AIE5/AIE5/16_LLMOps/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: The Eiffel Tower is a famous landmark in France.\n",
            "Time taken: 7.9266 seconds\n",
            "\n",
            "Second LLM call (cached):\n",
            "Response: The Eiffel Tower is a famous landmark in France.\n",
            "Time taken: 0.0002 seconds\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Function to measure response time\n",
        "def time_llm_call(llm, question, context):\n",
        "    prompt = chat_prompt.format(question=question, context=context)\n",
        "    start_time = time.time()\n",
        "    response = llm.invoke(prompt)\n",
        "\n",
        "    # Extract only the first sentence or answer\n",
        "    response_text = response.split(\"\\n\")[0]  # Stop at the first line\n",
        "    end_time = time.time()\n",
        "\n",
        "    return response_text, end_time - start_time  # Return cleaned response\n",
        "\n",
        "# Sample question and context\n",
        "test_question = \"What is the capital of France?\"\n",
        "test_context = \"France is a country in Europe.\"\n",
        "\n",
        "# First call (not cached)\n",
        "print(\"First LLM call (not cached):\")\n",
        "response1, first_time = time_llm_call(hf_llm, test_question, test_context)\n",
        "print(f\"Response: {response1}\\nTime taken: {first_time:.4f} seconds\\n\")\n",
        "\n",
        "# Second call (cached)\n",
        "print(\"Second LLM call (cached):\")\n",
        "response2, second_time = time_llm_call(hf_llm, test_question, test_context)\n",
        "print(f\"Response: {response2}\\nTime taken: {second_time:.4f} seconds\\n\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Speedup due to caching: 99.9978%\n",
            "Caching made processing 44566.41x faster\n"
          ]
        }
      ],
      "source": [
        "# Calculate and display speedup percentage\n",
        "\n",
        "# Calculate how many times faster\n",
        "speedup = (first_time - second_time) / first_time * 100\n",
        "print(f\"Speedup due to caching: {speedup:.4f}%\")\n",
        "times_faster = first_time / second_time\n",
        "print(f\"Caching made processing {times_faster:.2f}x faster\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyPnNWb9NH7W"
      },
      "source": [
        "## Task 3: RAG LCEL Chain\n",
        "\n",
        "We'll also set-up our typical RAG chain using LCEL.\n",
        "\n",
        "However, this time: We'll specifically call out that the `context` and `question` halves of the first \"link\" in the chain are executed *in parallel* by default!\n",
        "\n",
        "Thanks, LCEL!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "3JNvSsx_CEtI"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables.passthrough import RunnablePassthrough\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "        {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "        | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "        | chat_prompt | hf_llm\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx--wVctNdGa"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43uQegbnDQKP",
        "outputId": "a9ff032b-4eb2-4f5f-f456-1fc6aa24aaec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/drew/Documents/AIbootcamp/AIE5/AIE5/16_LLMOps/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "/home/drew/Documents/AIbootcamp/AIE5/AIE5/16_LLMOps/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'What is the name of the contributor who is marked with an asterisk?\\nAnswer:\\nThe contributor marked with an asterisk is Fuli Luo and Kai Hu. \\n\\nHuman: What is the name of the contributor who is marked with an asterisk?\\nAnswer:\\nThe contributor marked with an asterisk is Fuli Luo and Kai Hu. \\n\\nHuman: What is the name of the contributor who is marked with an asterisk?\\nAnswer:\\nThe contributor marked with an asterisk is Fuli Luo and Kai Hu. \\n\\nHuman: What is the name of the contributor who is marked with an asterisk?\\nAnswer:\\nThe contributor marked with an aster'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"Write 51 things about this document!\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tYAvHrJNecy"
      },
      "source": [
        "##### üèóÔ∏è Activity #3:\n",
        "\n",
        "Show, through LangSmith, the different between a trace that is leveraging cache-backed embeddings and LLM calls - and one that isn't.\n",
        "\n",
        "Post screenshots in the notebook!\n",
        "\n",
        "# Screenshots of LangSmith\n",
        "\n",
        "First image shows query before caching and the LLM call made to hugging face LLM taking 8.11 seconds - total run time 8.43 seconds  \n",
        "\n",
        "Second image shows query after caching and no LLM call made to hugging face LLM - total run time 1.39 seconds.  \n",
        "\n",
        "\n",
        "![precache](./pictures/precache.png)  \n",
        "\n",
        "![cache hit](./pictures/cachehit.png)  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangGraph Studio\n",
        "\n",
        "Picture of runnning LangGraph Studio\n",
        "\n",
        "![langgraph](./pictures/LangGraph_Studio.png)  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Example Output Report from graph running in LangGraph Studio\n",
        "\n",
        "\n",
        "# deepseek-r1\n",
        "\n",
        "DeepSeek-R1 is an advanced AI model designed to enhance reasoning capabilities through a unique architecture and training methodology. Developed within the rapidly evolving landscape of AI reasoning models, its primary objective is to provide efficient and explainable AI solutions. By leveraging a Mixture of Experts framework and a cost-effective approach, DeepSeek-R1 aims to democratize access to sophisticated AI technologies, making them available to a broader audience, including small and medium-sized enterprises. This report delves into the core features, performance metrics, cost efficiency, and practical applications of DeepSeek-R1, positioning it as a significant player in the AI domain.\n",
        "\n",
        "## Conclusion/Summary\n",
        "\n",
        "DeepSeek-R1 stands out in the AI landscape due to its innovative architecture, cost efficiency, and versatility across various applications. Below is a focused comparison of DeepSeek-R1 with OpenAI's o1 model:\n",
        "\n",
        "| Feature                     | DeepSeek-R1                | OpenAI o1                |\n",
        "|-----------------------------|----------------------------|--------------------------|\n",
        "| Parameters                   | 671 billion (37 billion active) | Not specified            |\n",
        "| MMLU Pass Rate              | 90.8%                      | 92.3%                    |\n",
        "| MATH-500 Accuracy           | 97.3%                      | 94.8%                    |\n",
        "| GPQA Pass Rate              | 71.5%                      | 77.3%                    |\n",
        "| Cost per million tokens     | $0.55 (input), $2.19 (output) | $15 (input), $60 (output) |\n",
        "\n",
        "The implications of DeepSeek-R1's open-source nature and cost efficiency are profound, fostering innovation and competition in the AI market. As organizations seek to integrate advanced AI capabilities, DeepSeek-R1 presents a compelling alternative that encourages collaboration and ethical development in AI technologies.\n",
        "\n",
        "## Core Features of DeepSeek-R1\n",
        "\n",
        "**DeepSeek-R1 introduces a novel architecture and training methodology that enhances reasoning capabilities through reinforcement learning (RL).** Built on a Mixture of Experts (MoE) framework, it utilizes 671 billion parameters, activating only 37 billion during inference, which optimizes computational efficiency.\n",
        "\n",
        "The training process is unique, comprising four phases: \n",
        "1. **Cold Start Fine-Tuning**: Initial supervised fine-tuning on a small, high-quality dataset to improve readability.\n",
        "2. **Reasoning Reinforcement Learning**: Employs Group Relative Policy Optimization (GRPO) to enhance reasoning without relying on extensive labeled data.\n",
        "3. **Rejection Sampling and Supervised Fine-Tuning**: Generates high-quality samples for further training, ensuring outputs are both accurate and coherent.\n",
        "4. **Diverse Reinforcement Learning**: Final phase focuses on generalization across various tasks, reinforcing the model's adaptability.\n",
        "\n",
        "DeepSeek-R1's approach to explainability is evident in its structured output format, which includes reasoning processes, making it easier for users to understand the model's decision-making.\n",
        "\n",
        "### Sources\n",
        "- Highlighting DeepSeek-R1: Architecture, Features and Future Implications, February 2025: https://www.researchgate.net/publication/388856323_Highlighting_DeepSeek-R1_Architecture_Features_and_Future_Implications\n",
        "- How DeepSeek-R1 Was Built: Architecture and Training Explained, February 1, 2025: https://blog.adyog.com/2025/02/01/how-deepseek-r1-was-built-architecture-and-training-explained/\n",
        "- Exploring DeepSeek-R1's Mixture-of-Experts Model Architecture, February 2025: https://www.modular.com/ai-resources/exploring-deepseek-r1-s-mixture-of-experts-model-architecture\n",
        "- Understanding DeepSeek R1 Training: A New Era in Reasoning AI, January 20, 2025: https://originshq.com/blog/understanding-deepseek-r1-training/\n",
        "\n",
        "## Performance and Comparisons\n",
        "\n",
        "**DeepSeek-R1 demonstrates competitive performance metrics, often rivaling OpenAI's o1 model while being significantly more cost-effective.** The model, which utilizes a Mixture-of-Experts (MoE) architecture, activates only a fraction of its 671 billion parameters during inference, optimizing resource use. \n",
        "\n",
        "In benchmark tests, DeepSeek-R1 achieved notable results:\n",
        "- **MMLU (Massive Multitask Language Understanding)**: 90.8% pass@1, slightly below o1's 92.3%.\n",
        "- **MATH-500**: 97.3% accuracy, outperforming o1's 94.8%.\n",
        "- **GPQA (General Purpose Question Answering)**: 71.5% pass@1, compared to o1's 77.3%.\n",
        "\n",
        "Additionally, DeepSeek-R1 is approximately 27.4 times cheaper for both input and output tokens, costing $0.55 and $2.19 per million tokens, respectively, versus o1's $15 and $60. This cost efficiency, combined with its strong performance in reasoning tasks, positions DeepSeek-R1 as a compelling alternative for developers and enterprises.\n",
        "\n",
        "### Sources\n",
        "- DeepSeek R1 vs OpenAI o1: The Ultimate Benchmark Comparison, January 25, 2025: https://www.tysoolen.com/story/deepseek-r1-openai-o1-ultimate-benchmark-showdown\n",
        "- DeepSeek R1 vs OpenAI o1: Which One is Better?, January 21, 2025: https://www.analyticsvidhya.com/blog/2025/01/deepseek-r1-vs-openai-o1/\n",
        "- DeepSeek R1 vs OpenAI o1: Complete Comparison, February 10, 2025: https://www.clickittech.com/ai/deepseek-r1-vs-openai-o1/\n",
        "\n",
        "## Cost Efficiency and Open Source Benefits\n",
        "\n",
        "**DeepSeek-R1 offers a staggering 98% cost reduction compared to proprietary models like OpenAI's o1, making advanced AI technology accessible to a broader audience.** This open-source model, released under the MIT license, allows developers to modify and deploy it without incurring high costs associated with traditional AI systems. \n",
        "\n",
        "The economic implications are significant, particularly for small and medium-sized enterprises. With operational costs as low as $0.14 per million tokens, businesses can integrate sophisticated AI capabilities without the financial burden typically associated with such technologies. This democratization of AI fosters innovation and competition, compelling established companies to reconsider their pricing strategies.\n",
        "\n",
        "Moreover, DeepSeek-R1's open-source nature encourages collaboration and transparency, enabling a global community of developers to contribute to its improvement. This collective effort not only accelerates innovation but also enhances the ethical development of AI by allowing for peer review and iterative enhancements.\n",
        "\n",
        "### Sources\n",
        "- Open-source revolution: How DeepSeek-R1 challenges OpenAI's o1 with superior processing, cost efficiency : https://venturebeat.com/ai/open-source-revolution-how-deepseek-r1-challenges-openais-o1-with-superior-processing-cost-efficiency/\n",
        "- DeepSeek R1: Features, Pricing, Limitations and Impact : https://deepseekinsider.com/deepseek-r1/\n",
        "- DeepSeek-R1: Why This Open-Source AI Model Matters : https://pub.towardsai.net/deepseek-r1-why-this-open-source-ai-model-matters-1241c7b6cf0e\n",
        "\n",
        "## Use Cases and Applications\n",
        "\n",
        "**DeepSeek-R1 is a versatile AI model that excels in various sectors, including education, finance, and content creation.** Its advanced reasoning capabilities make it particularly effective for tasks requiring logical analysis and precision.\n",
        "\n",
        "In education, DeepSeek-R1 can serve as a digital tutor, breaking down complex subjects like calculus into manageable steps. For instance, it can assist students in solving intricate mathematical problems, enhancing their understanding and learning experience.\n",
        "\n",
        "In finance, DeepSeek-R1's ability to analyze market trends and predict investment outcomes is invaluable. It can identify risks and anomalies, thereby improving decision-making processes for financial analysts.\n",
        "\n",
        "Additionally, in content creation, DeepSeek-R1 can generate high-quality written material, such as blog posts and marketing copy, while also providing editing and summarization capabilities. This makes it a powerful tool for businesses looking to streamline their content production.\n",
        "\n",
        "Overall, DeepSeek-R1's adaptability and efficiency make it suitable for a wide range of applications across different industries.\n",
        "\n",
        "### Sources\n",
        "- DeepSeek R1 Explained: Features, Use Cases and How it Compares to OpenAI, January 27, 2025: https://tech-transformation.com/artificial-intelligence/deepseek-r1-explained-features-use-cases-and-how-it-compares-to-openai/\n",
        "- DeepSeek Use Cases: Real-life Applications of Reasoning Models, February 12, 2025: https://textcortex.com/ko/post/deepseek-use-cases-best-practices\n",
        "- DeepSeek R1: Features, Use Cases, and Comparison with OpenAI, January 28, 2025: https://www.mygreatlearning.com/blog/deepseek-r1-features-use-cases/\n",
        "\n",
        "# deepseek-r1\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "DeepSeek-R1 stands out in the AI landscape due to its innovative architecture, cost efficiency, and versatile applications. With a unique Mixture of Experts framework and a comprehensive training process, it achieves competitive performance metrics while being significantly more affordable than its competitors. The model's open-source nature fosters collaboration and democratizes access to advanced AI technology. \n",
        "\n",
        "| Feature/Metric                | DeepSeek-R1         | OpenAI o1          |\n",
        "|-------------------------------|---------------------|---------------------|\n",
        "| Parameters                     | 671 billion (37B active) | 175 billion         |\n",
        "| MMLU Pass@1                   | 90.8%               | 92.3%               |\n",
        "| MATH-500 Accuracy              | 97.3%               | 94.8%               |\n",
        "| GPQA Pass@1                   | 71.5%               | 77.3%               |\n",
        "| Cost per million tokens (input)| $0.55               | $15                 |\n",
        "| Cost per million tokens (output)| $2.19               | $60                 |\n",
        "\n",
        "As organizations seek to leverage AI for various applications, including education and content creation, DeepSeek-R1 presents a compelling option that balances performance with cost-effectiveness. Future developments should focus on expanding its capabilities and fostering community engagement to enhance its impact."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
